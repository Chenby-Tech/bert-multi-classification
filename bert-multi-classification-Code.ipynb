{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0ba6c5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "clean_up_tokenization_spaces = True\n",
    "\n",
    "#加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15f0e1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 45000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 5000\n",
       " }))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset,list_metrics\n",
    "\n",
    "#加载数据集\n",
    "dataset = load_dataset('csv', data_files='input_your_tran_dataset.csv', split='train')\n",
    "dataset2 = load_dataset('csv', data_files='input_your_test_dataset.csv', split='train')\n",
    "\n",
    "\n",
    "dataset,dataset2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "213f86ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([16, 32])\n",
      "attention_mask torch.Size([16, 32])\n",
      "label torch.Size([16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2812"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义数据集遍历工具\n",
    "def collate_fn(data):\n",
    "    text = [i['text'] for i in data]\n",
    "    label = [i['label'] for i in data]\n",
    "\n",
    "    #文字编码\n",
    "    data = tokenizer(text,\n",
    "                     padding=True,\n",
    "                     truncation=True,\n",
    "                     max_length=500,\n",
    "                     return_tensors='pt',\n",
    "                     return_token_type_ids=False)\n",
    "\n",
    "    #设置label\n",
    "    data['label'] = torch.LongTensor(label)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=16,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True,\n",
    "                                     collate_fn=collate_fn)\n",
    "\n",
    "data = next(iter(loader))\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ec109537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.5774, grad_fn=<NllLossBackward0>),\n",
       " tensor([[0.1326, 0.0947, 0.3107, 0.0700, 0.3920],\n",
       "         [0.1602, 0.1156, 0.3069, 0.1552, 0.2621],\n",
       "         [0.1686, 0.1709, 0.2465, 0.1592, 0.2548],\n",
       "         [0.1389, 0.2530, 0.2221, 0.1494, 0.2366],\n",
       "         [0.1646, 0.1782, 0.2519, 0.1446, 0.2607],\n",
       "         [0.1081, 0.1388, 0.1878, 0.2193, 0.3461],\n",
       "         [0.1370, 0.1442, 0.2241, 0.1694, 0.3253],\n",
       "         [0.2040, 0.1116, 0.2902, 0.1238, 0.2704],\n",
       "         [0.1699, 0.1952, 0.2487, 0.1516, 0.2346],\n",
       "         [0.1314, 0.0869, 0.3251, 0.1338, 0.3228],\n",
       "         [0.1588, 0.1595, 0.3180, 0.1217, 0.2421],\n",
       "         [0.1491, 0.0854, 0.3384, 0.1225, 0.3046],\n",
       "         [0.1103, 0.2380, 0.2126, 0.1618, 0.2773],\n",
       "         [0.1630, 0.1464, 0.2718, 0.1444, 0.2745],\n",
       "         [0.2125, 0.0777, 0.2353, 0.1508, 0.3238],\n",
       "         [0.1560, 0.1113, 0.2628, 0.1964, 0.2736]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义模型\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #加载预训练模型\n",
    "        from transformers import AutoModel\n",
    "        self.pretrained = AutoModel.from_pretrained(\n",
    "            'google-bert/bert-base-chinese')\n",
    "\n",
    "        #self.fc = torch.nn.Linear(in_features=768, out_features=2)\n",
    "        self.fc = torch.nn.Linear(in_features=768, out_features=5)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label=None):\n",
    "        #使用预训练模型抽取数据特征\n",
    "        with torch.no_grad():\n",
    "            last_hidden_state = self.pretrained(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        #只取第0个词的特征做分类\n",
    "        last_hidden_state = last_hidden_state[:, 0]\n",
    "\n",
    "        #对抽取的特征只取第一个字的结果做分类即可\n",
    "        out = self.fc(last_hidden_state).softmax(dim=1)\n",
    "\n",
    "        #计算loss\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(out, label)\n",
    "\n",
    "        return loss, out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "762606f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2812 1.0746947526931763 0.875\n",
      "100 2812 1.015134572982788 0.875\n",
      "200 2812 1.1222753524780273 0.75\n",
      "300 2812 0.9760445952415466 0.9375\n",
      "400 2812 0.9791123270988464 0.9375\n",
      "500 2812 1.0803909301757812 0.8125\n",
      "600 2812 1.0309051275253296 0.875\n",
      "700 2812 0.9435992240905762 1.0\n",
      "800 2812 1.1437382698059082 0.8125\n",
      "900 2812 0.9202396869659424 1.0\n",
      "1000 2812 1.2638517618179321 0.625\n",
      "1100 2812 0.9314250349998474 1.0\n",
      "1200 2812 1.0861499309539795 0.8125\n",
      "1300 2812 1.0201774835586548 0.9375\n",
      "1400 2812 1.1565898656845093 0.75\n",
      "1500 2812 1.0710444450378418 0.8125\n",
      "1600 2812 1.0073071718215942 0.875\n",
      "1700 2812 0.9736364483833313 0.9375\n",
      "1800 2812 1.221720814704895 0.6875\n",
      "1900 2812 1.087439775466919 0.8125\n",
      "2000 2812 1.0330674648284912 0.875\n",
      "2100 2812 1.0394777059555054 0.875\n",
      "2200 2812 0.9726205468177795 0.9375\n",
      "2300 2812 0.9983895421028137 0.9375\n",
      "2400 2812 1.0573872327804565 0.8125\n",
      "2500 2812 1.0396744012832642 1.0\n",
      "2600 2812 1.041595697402954 0.875\n",
      "2700 2812 1.0770405530929565 0.8125\n",
      "2800 2812 1.1819407939910889 0.6875\n",
      "report:               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.88      0.85      8999\n",
      "         1.0       0.87      0.84      0.85      8998\n",
      "         2.0       0.91      0.87      0.89      8999\n",
      "         3.0       0.84      0.85      0.84      8997\n",
      "         4.0       0.83      0.83      0.83      8999\n",
      "\n",
      "    accuracy                           0.85     44992\n",
      "   macro avg       0.85      0.85      0.85     44992\n",
      "weighted avg       0.85      0.85      0.85     44992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#执行训练\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "def train():\n",
    "    y_true_all = torch.tensor([])\n",
    "    y_pred_all = torch.tensor([])\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "    for i, data in enumerate(loader):\n",
    "        loss, out = model(**data)\n",
    "        \n",
    "        y_true = data.label\n",
    "        #print(f'y_true: {y_true}')\n",
    "        y_pred = out.argmax(dim=1)\n",
    "        #print(f'y_pred: {y_pred}')\n",
    "        y_true_all = torch.cat((y_true_all, y_true), dim=0)\n",
    "        y_pred_all = torch.cat((y_pred_all, y_pred), dim=0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            out = out.argmax(dim=1)\n",
    "            acc = (out == data.label).sum().item() / len(data.label)\n",
    "            print(i, len(loader), loss.item(), acc)\n",
    "\n",
    "        #if i == 50:\n",
    "        #    break\n",
    "    report = classification_report(y_true_all, y_pred_all)  \n",
    "    print(f'report: {report}')\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a589baf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 312 0.9375\n",
      "1 312 0.84375\n",
      "2 312 0.8125\n",
      "3 312 0.84375\n",
      "4 312 0.8625\n",
      "5 312 0.875\n",
      "6 312 0.8660714285714286\n",
      "7 312 0.8671875\n",
      "8 312 0.875\n",
      "9 312 0.875\n",
      "10 312 0.8693181818181818\n",
      "11 312 0.859375\n",
      "12 312 0.8653846153846154\n",
      "13 312 0.8660714285714286\n",
      "14 312 0.8666666666666667\n",
      "15 312 0.85546875\n",
      "16 312 0.8602941176470589\n",
      "17 312 0.8576388888888888\n",
      "18 312 0.8519736842105263\n",
      "19 312 0.85625\n",
      "20 312 0.8571428571428571\n",
      "21 312 0.8579545454545454\n",
      "22 312 0.8586956521739131\n",
      "23 312 0.859375\n",
      "24 312 0.8625\n",
      "25 312 0.8605769230769231\n",
      "26 312 0.8587962962962963\n",
      "27 312 0.8616071428571429\n",
      "28 312 0.8577586206896551\n",
      "29 312 0.8583333333333333\n",
      "30 312 0.8608870967741935\n",
      "31 312 0.859375\n",
      "32 312 0.8636363636363636\n",
      "33 312 0.8658088235294118\n",
      "34 312 0.8660714285714286\n",
      "35 312 0.8680555555555556\n",
      "36 312 0.8699324324324325\n",
      "37 312 0.8601973684210527\n",
      "38 312 0.8589743589743589\n",
      "39 312 0.859375\n",
      "40 312 0.8612804878048781\n",
      "41 312 0.8601190476190477\n",
      "42 312 0.8604651162790697\n",
      "43 312 0.859375\n",
      "44 312 0.8597222222222223\n",
      "45 312 0.8627717391304348\n",
      "46 312 0.8643617021276596\n",
      "47 312 0.859375\n",
      "48 312 0.8584183673469388\n",
      "49 312 0.85625\n",
      "50 312 0.8566176470588235\n",
      "51 312 0.8557692307692307\n",
      "52 312 0.8525943396226415\n",
      "53 312 0.8530092592592593\n",
      "54 312 0.8545454545454545\n",
      "55 312 0.8549107142857143\n",
      "56 312 0.8552631578947368\n",
      "57 312 0.8556034482758621\n",
      "58 312 0.8559322033898306\n",
      "59 312 0.8541666666666666\n",
      "60 312 0.8565573770491803\n",
      "61 312 0.8568548387096774\n",
      "62 312 0.8581349206349206\n",
      "63 312 0.8583984375\n",
      "64 312 0.8596153846153847\n",
      "65 312 0.8598484848484849\n",
      "66 312 0.8610074626865671\n",
      "67 312 0.8612132352941176\n",
      "68 312 0.8605072463768116\n",
      "69 312 0.8607142857142858\n",
      "70 312 0.8617957746478874\n",
      "71 312 0.8628472222222222\n",
      "72 312 0.8621575342465754\n",
      "73 312 0.8606418918918919\n",
      "74 312 0.8625\n",
      "75 312 0.8626644736842105\n",
      "76 312 0.8628246753246753\n",
      "77 312 0.8637820512820513\n",
      "78 312 0.8647151898734177\n",
      "79 312 0.8640625\n",
      "80 312 0.8641975308641975\n",
      "81 312 0.8650914634146342\n",
      "82 312 0.8629518072289156\n",
      "83 312 0.8616071428571429\n",
      "84 312 0.8610294117647059\n",
      "85 312 0.8619186046511628\n",
      "86 312 0.8627873563218391\n",
      "87 312 0.8636363636363636\n",
      "88 312 0.8644662921348315\n",
      "89 312 0.8645833333333334\n",
      "90 312 0.8646978021978022\n",
      "91 312 0.8648097826086957\n",
      "92 312 0.8635752688172043\n",
      "93 312 0.863031914893617\n",
      "94 312 0.8618421052631579\n",
      "95 312 0.8626302083333334\n",
      "96 312 0.8634020618556701\n",
      "97 312 0.8635204081632653\n",
      "98 312 0.86489898989899\n",
      "99 312 0.865625\n",
      "100 312 0.8657178217821783\n",
      "101 312 0.866421568627451\n",
      "102 312 0.8658980582524272\n",
      "103 312 0.8665865384615384\n",
      "104 312 0.8666666666666667\n",
      "105 312 0.8661556603773585\n",
      "106 312 0.8662383177570093\n",
      "107 312 0.8663194444444444\n",
      "108 312 0.8658256880733946\n",
      "109 312 0.8664772727272727\n",
      "110 312 0.8665540540540541\n",
      "111 312 0.8649553571428571\n",
      "112 312 0.8661504424778761\n",
      "113 312 0.8651315789473685\n",
      "114 312 0.8657608695652174\n",
      "115 312 0.8663793103448276\n",
      "116 312 0.8664529914529915\n",
      "117 312 0.8659957627118644\n",
      "118 312 0.8660714285714286\n",
      "119 312 0.865625\n",
      "120 312 0.8657024793388429\n",
      "121 312 0.8642418032786885\n",
      "122 312 0.864329268292683\n",
      "123 312 0.8649193548387096\n",
      "124 312 0.865\n",
      "125 312 0.8645833333333334\n",
      "126 312 0.8631889763779528\n",
      "127 312 0.86376953125\n",
      "128 312 0.8648255813953488\n",
      "129 312 0.8658653846153846\n",
      "130 312 0.8664122137404581\n",
      "131 312 0.8664772727272727\n",
      "132 312 0.8674812030075187\n",
      "133 312 0.867070895522388\n",
      "134 312 0.8680555555555556\n",
      "135 312 0.8685661764705882\n",
      "136 312 0.8690693430656934\n",
      "137 312 0.8682065217391305\n",
      "138 312 0.8682553956834532\n",
      "139 312 0.86875\n",
      "140 312 0.8692375886524822\n",
      "141 312 0.8683978873239436\n",
      "142 312 0.8684440559440559\n",
      "143 312 0.8689236111111112\n",
      "144 312 0.8689655172413793\n",
      "145 312 0.8685787671232876\n",
      "146 312 0.8681972789115646\n",
      "147 312 0.8673986486486487\n",
      "148 312 0.8670302013422819\n",
      "149 312 0.8679166666666667\n",
      "150 312 0.8683774834437086\n",
      "151 312 0.8692434210526315\n",
      "152 312 0.8688725490196079\n",
      "153 312 0.8685064935064936\n",
      "154 312 0.8689516129032258\n",
      "155 312 0.8693910256410257\n",
      "156 312 0.8698248407643312\n",
      "157 312 0.8698575949367089\n",
      "158 312 0.8702830188679245\n",
      "159 312 0.869921875\n",
      "160 312 0.8699534161490683\n",
      "161 312 0.8699845679012346\n",
      "162 312 0.8696319018404908\n",
      "163 312 0.8696646341463414\n",
      "164 312 0.8693181818181818\n",
      "165 312 0.870105421686747\n",
      "166 312 0.8701347305389222\n",
      "167 312 0.8701636904761905\n",
      "168 312 0.8694526627218935\n",
      "169 312 0.8698529411764706\n",
      "170 312 0.8695175438596491\n",
      "171 312 0.8691860465116279\n",
      "172 312 0.8692196531791907\n",
      "173 312 0.8688936781609196\n",
      "174 312 0.8689285714285714\n",
      "175 312 0.8693181818181818\n",
      "176 312 0.8700564971751412\n",
      "177 312 0.8700842696629213\n",
      "178 312 0.8701117318435754\n",
      "179 312 0.8701388888888889\n",
      "180 312 0.8708563535911602\n",
      "181 312 0.8695054945054945\n",
      "182 312 0.8688524590163934\n",
      "183 312 0.8692255434782609\n",
      "184 312 0.8689189189189189\n",
      "185 312 0.8682795698924731\n",
      "186 312 0.8683155080213903\n",
      "187 312 0.8686835106382979\n",
      "188 312 0.8683862433862434\n",
      "189 312 0.86875\n",
      "190 312 0.868128272251309\n",
      "191 312 0.8678385416666666\n",
      "192 312 0.868199481865285\n",
      "193 312 0.8682345360824743\n",
      "194 312 0.867948717948718\n",
      "195 312 0.8679846938775511\n",
      "196 312 0.8683375634517766\n",
      "197 312 0.8674242424242424\n",
      "198 312 0.8671482412060302\n",
      "199 312 0.8671875\n",
      "200 312 0.8675373134328358\n",
      "201 312 0.8672648514851485\n",
      "202 312 0.8673029556650246\n",
      "203 312 0.8667279411764706\n",
      "204 312 0.8670731707317073\n",
      "205 312 0.8677184466019418\n",
      "206 312 0.8671497584541062\n",
      "207 312 0.8677884615384616\n",
      "208 312 0.868421052631579\n",
      "209 312 0.868452380952381\n",
      "210 312 0.8681872037914692\n",
      "211 312 0.8676297169811321\n",
      "212 312 0.8679577464788732\n",
      "213 312 0.8676985981308412\n",
      "214 312 0.8674418604651163\n",
      "215 312 0.8666087962962963\n",
      "216 312 0.8663594470046083\n",
      "217 312 0.8663990825688074\n",
      "218 312 0.8661529680365296\n",
      "219 312 0.865909090909091\n",
      "220 312 0.8659502262443439\n",
      "221 312 0.8657094594594594\n",
      "222 312 0.8657511210762332\n",
      "223 312 0.8660714285714286\n",
      "224 312 0.8658333333333333\n",
      "225 312 0.8655973451327433\n",
      "226 312 0.865363436123348\n",
      "227 312 0.8656798245614035\n",
      "228 312 0.8659934497816594\n",
      "229 312 0.866304347826087\n",
      "230 312 0.8666125541125541\n",
      "231 312 0.8671875\n",
      "232 312 0.8669527896995708\n",
      "233 312 0.8672542735042735\n",
      "234 312 0.8675531914893617\n",
      "235 312 0.8678495762711864\n",
      "236 312 0.8681434599156118\n",
      "237 312 0.868172268907563\n",
      "238 312 0.8676778242677824\n",
      "239 312 0.8677083333333333\n",
      "240 312 0.8674792531120332\n",
      "241 312 0.8677685950413223\n",
      "242 312 0.8683127572016461\n",
      "243 312 0.8685963114754098\n",
      "244 312 0.8681122448979591\n",
      "245 312 0.868140243902439\n",
      "246 312 0.867661943319838\n",
      "247 312 0.8679435483870968\n",
      "248 312 0.8679718875502008\n",
      "249 312 0.86825\n",
      "250 312 0.8682768924302788\n",
      "251 312 0.8675595238095238\n",
      "252 312 0.8680830039525692\n",
      "253 312 0.8678641732283464\n",
      "254 312 0.8676470588235294\n",
      "255 312 0.867919921875\n",
      "256 312 0.8679474708171206\n",
      "257 312 0.8679748062015504\n",
      "258 312 0.8677606177606177\n",
      "259 312 0.8673076923076923\n",
      "260 312 0.8666187739463601\n",
      "261 312 0.8664122137404581\n",
      "262 312 0.8662072243346007\n",
      "263 312 0.8660037878787878\n",
      "264 312 0.8665094339622641\n",
      "265 312 0.8663063909774437\n",
      "266 312 0.8665730337078652\n",
      "267 312 0.8663712686567164\n",
      "268 312 0.8664033457249071\n",
      "269 312 0.8668981481481481\n",
      "270 312 0.8669280442804428\n",
      "271 312 0.8669577205882353\n",
      "272 312 0.8669871794871795\n",
      "273 312 0.8667883211678832\n",
      "274 312 0.8672727272727273\n",
      "275 312 0.8677536231884058\n",
      "276 312 0.8671028880866426\n",
      "277 312 0.866681654676259\n",
      "278 312 0.866263440860215\n",
      "279 312 0.8662946428571429\n",
      "280 312 0.8663256227758007\n",
      "281 312 0.8661347517730497\n",
      "282 312 0.8659452296819788\n",
      "283 312 0.8657570422535211\n",
      "284 312 0.8655701754385965\n",
      "285 312 0.8658216783216783\n",
      "286 312 0.8656358885017421\n",
      "287 312 0.8656684027777778\n",
      "288 312 0.8652681660899654\n",
      "289 312 0.865301724137931\n",
      "290 312 0.8651202749140894\n",
      "291 312 0.865154109589041\n",
      "292 312 0.8649744027303754\n",
      "293 312 0.8650085034013606\n",
      "294 312 0.8648305084745763\n",
      "295 312 0.8646537162162162\n",
      "296 312 0.8646885521885522\n",
      "297 312 0.8647231543624161\n",
      "298 312 0.8641304347826086\n",
      "299 312 0.8639583333333334\n",
      "300 312 0.8637873754152824\n",
      "301 312 0.8640314569536424\n",
      "302 312 0.8640676567656765\n",
      "303 312 0.864514802631579\n",
      "304 312 0.8647540983606558\n",
      "305 312 0.8649918300653595\n",
      "306 312 0.8654315960912052\n",
      "307 312 0.8654626623376623\n",
      "308 312 0.8656957928802589\n",
      "309 312 0.8659274193548387\n",
      "310 312 0.8655546623794212\n",
      "311 312 0.8657852564102564\n",
      "report:               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.87      0.86       999\n",
      "         1.0       0.86      0.85      0.85       997\n",
      "         2.0       0.91      0.87      0.89      1000\n",
      "         3.0       0.85      0.86      0.85       998\n",
      "         4.0       0.86      0.88      0.87       998\n",
      "\n",
      "    accuracy                           0.87      4992\n",
      "   macro avg       0.87      0.87      0.87      4992\n",
      "weighted avg       0.87      0.87      0.87      4992\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8657852564102564"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#执行测试\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "def test():\n",
    "    #loader_test = torch.utils.data.DataLoader(dataset['test'],\n",
    "    loader_test = torch.utils.data.DataLoader(dataset2,\n",
    "                                              batch_size=16,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True,\n",
    "                                              collate_fn=collate_fn)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true_all = torch.tensor([])\n",
    "    y_pred_all = torch.tensor([])\n",
    "    for i, data in enumerate(loader_test):\n",
    "        with torch.no_grad():\n",
    "            _, out = model(**data)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "\n",
    "        #模型评价函数\n",
    "        y_true = data.label\n",
    "        y_pred = out\n",
    "        y_true_all = torch.cat((y_true_all, y_true), dim=0)\n",
    "        y_pred_all = torch.cat((y_pred_all, y_pred), dim=0)\n",
    "        \n",
    "        correct += (out == data.label).sum().item()\n",
    "        \n",
    "        total += len(data.label)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(i, len(loader_test), correct / total)\n",
    "\n",
    "        #if i == 5:\n",
    "        #    break\n",
    "\n",
    "    report = classification_report(y_true_all, y_pred_all)  \n",
    "    print(f'report: {report}')\n",
    "    return correct / total\n",
    "\n",
    "    \n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db98ab7-33fb-4d4b-98d9-3b73cbd2e9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
